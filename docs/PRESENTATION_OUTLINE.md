# Health Eat 발표자료 구조

> **발표 시간**: 15분 | **대상**: 멘토님

---

## 슬라이드 구성 (총 15-18장)

---

### 1. 표지 (1장)

```
Health Eat - AI 알약 인식 프로젝트

코드잇 8팀
2025.12.04 ~ 12.23

팀원: 이진석(Leader), 김민우, 김나연, 김보윤, 황유민
```

---

### 2. 목차 (1장)

```
1. 프로젝트 개요
2. 데이터 분석
3. 접근 방식의 변화
4. 최종 모델 아키텍처
5. 핵심 기술 상세
6. 실험 결과
7. 팀 협업
8. 결론 및 회고
```

---

### 3. 프로젝트 개요 (1장)

**핵심 내용**:
| 항목 | 내용 |
|------|------|
| 목표 | Kaggle mAP@[0.75:0.95] 최대화 |
| 기간 | 3주 (12/04 ~ 12/23) |
| 최종 점수 | **0.96703** |
| Baseline 대비 | **+18.7%** 개선 |

**시각 자료**: 점수 개선 그래프
```
0.815 → 0.690 → 0.920 → 0.963 → 0.96703
```

---

### 4. 데이터 분석 - 문제 인식 (1장)

**핵심 발견**:

| 구분 | 학습 데이터 | 테스트 데이터 |
|------|------------|--------------|
| 이미지 수 | 232개 | 843개 |
| 클래스 수 | 74개 | 196개 |

**문제점**:
- 클래스 불일치 (74 vs 196)
- 심각한 클래스 불균형 (1:80 비율)
- 이미지 수 부족 (232개)

**시각 자료**: 클래스 분포 막대 그래프

---

### 5. 데이터 분석 - 해결 전략 (1장)

**외부 데이터 활용**: AIHub 의약품 데이터셋

| 데이터 | 용도 | 이미지 수 |
|--------|------|----------|
| Kaggle | Classifier 학습 | 232개 |
| AIHub | Detector 학습 | ~7,000개 |

**데이터 필터링 전략**:
- 테스트와 유사한 3~4개 bbox 이미지만 사용
- 모든 알약의 bbox 수집 (핵심 개선점)

---

### 6. 접근 방식의 변화 (2장)

#### 6-1. 시도와 실패

| 시도 | 결과 | 문제점 |
|------|------|--------|
| End-to-end YOLO (74 클래스) | 0.815 | 클래스 수 제한 |
| End-to-end YOLO (196 클래스) | 0.690 | 학습 데이터 부족 |
| AIHub 콤보 데이터 추가 | 0.6 | 미라벨링 객체 → 배경 학습 |

**핵심 교훈**:
- 데이터 품질 > 데이터 양
- End-to-end 방식의 한계 인식

#### 6-2. 전환점: 2-Stage 아이디어

**Before**:
```
이미지 → YOLO → 196개 클래스 직접 예측
```

**After**:
```
이미지 → Detector → 알약 위치 검출
       → Classifier → 74개 클래스 분류
```

**장점**:
1. Detection과 Classification 분리 → 각각 최적화
2. Detector는 단일 클래스 → 일반화 용이
3. Classifier는 크롭 이미지 → 분류 정확도 향상

---

### 7. 최종 모델 아키텍처 (1장)

```
┌─────────────────────────────────────────────────────────────┐
│                    2-Stage Pipeline                         │
├─────────────────────────────────────────────────────────────┤
│  Stage 1: YOLO11m Detector                                  │
│  ├── Input: 원본 이미지 (1280x960)                          │
│  ├── Output: Bounding Boxes                                 │
│  ├── 클래스: 단일 ("Pill")                                  │
│  └── 데이터: Kaggle + AIHub (~7,000개)                      │
├─────────────────────────────────────────────────────────────┤
│  Stage 2: ConvNeXt Classifier                               │
│  ├── Input: 크롭된 알약 이미지 (224x224)                    │
│  ├── Output: K-code + Confidence                            │
│  ├── 클래스: 74개                                           │
│  └── Pretrained: ImageNet                                   │
└─────────────────────────────────────────────────────────────┘
```

---

### 8. 핵심 기술 상세 - Stage 1 (1장)

**YOLO11m Detector**

| 설정 | 값 |
|------|-----|
| 모델 | yolo11m.pt |
| imgsz | 640 |
| epochs | 50 |
| batch | 8 |

**학습 결과**:
| Metric | 값 |
|--------|-----|
| mAP50 | 0.995 |
| mAP50-95 | 0.85 |
| Precision | 0.99 |
| Recall | 0.99 |

**핵심 개선**: AIHub bbox 추출 로직 수정
- 기존: 타겟 74개 클래스 폴더만 스캔 → bbox 누락
- 수정: 모든 K-code 폴더 스캔 → 전체 bbox 수집

---

### 9. 핵심 기술 상세 - Stage 2 (1장)

**ConvNeXt Classifier**

| 설정 | 값 |
|------|-----|
| 모델 | convnext_tiny (ImageNet pretrained) |
| img_size | 224 |
| epochs | 50 |
| lr | 1e-4 |

**선정 이유**:
1. ImageNet pretrained → 강력한 특징 추출
2. YOLO-cls 대비 분류 정확도 향상 (0.920 → 0.963)
3. 크롭 이미지 분류에 최적화

**학습 결과**:
- Val Accuracy: 98.5%
- Early Stop: Epoch 21

---

### 10. 추론 파이프라인 (1장)

```python
# 최적 설정
detector_conf = 0.05    # 낮춰서 recall 확보
classifier_conf = 0.3   # 분류 신뢰도
detector_iou = 0.5      # NMS 임계값
max_det = 4             # 이미지당 최대 검출
```

**추론 흐름**:
```
1. 이미지 로드
2. Detector 추론 → bbox 검출
3. bbox별 알약 크롭
4. Classifier 추론 → 클래스 예측
5. 결과 병합 → CSV 출력
```

---

### 11. 실험 결과 (1장)

**Kaggle 제출 히스토리**:

| # | Score | 설명 | 변화 |
|---|-------|------|------|
| 1 | 0.815 | Baseline (End-to-end) | - |
| 2 | 0.690 | 196 클래스 시도 | -0.125 |
| 3 | 0.920 | 2-Stage (YOLO-cls) | +0.230 |
| 4 | 0.963 | 2-Stage (ConvNeXt) | +0.043 |
| 5 | 0.965 | AIHub 데이터 추가 | +0.002 |
| **6** | **0.967** | **bbox 로직 수정** | **+0.002** |

**시각 자료**: 점수 추이 그래프

---

### 12. 실패 사례와 교훈 (1장)

| 시도 | 결과 | 교훈 |
|------|------|------|
| imgsz 1280 | 0.713 | 학습/추론 설정 일치 중요 |
| TTA 적용 | 0.533 | mAP@[0.75:0.95]에서 bbox 정밀도 저하 |
| 콤보 데이터 | 0.6 | 미라벨링 객체 → 성능 하락 |

**핵심 교훈**:
1. 데이터 품질 > 데이터 양
2. 실패도 중요한 학습 기회
3. 빠른 실험 → 빠른 피드백 → 방향 전환

---

### 13. 팀 협업 (1장)

**역할 분담**:

| 역할 | 담당자 | 주요 기여 |
|------|--------|----------|
| Leader | 이진석 | 2-Stage Pipeline, 통합 |
| Data Engineer | 김민우 | YOLO 데이터셋, 증강 |
| Data Engineer | 김나연 | EDA, 전처리 모듈화 |
| Model Architect | 김보윤 | 모델 설계, 자동화 |
| Experiment Lead | 황유민 | W&B, 실험 추적 |

**협업 통계**:
| 항목 | 수치 |
|------|------|
| 총 커밋 | ~180개 |
| PR | 86개 |
| 협업일지 | 40개 |
| 실험 로그 | 12개 |

---

### 14. 프로젝트 타임라인 (1장)

```
Week 0 (12/04~05): 셋업, EDA
    ↓
Week 1 (12/08~12): Baseline 0.815 제출
    ↓
Week 2 (12/15~18): 2-Stage 전환 → 0.963
    ↓
Week 3 (12/19~22): 최적화 → 0.96703
```

**핵심 전환점**: 12/18 - 2-Stage Pipeline 도입

---

### 15. 핵심 성공 요인 (1장)

1. **2-Stage 분리**
   - Detection과 Classification 독립 최적화
   - 각 Task에 맞는 모델 선택

2. **AIHub 데이터 활용**
   - 7,000개 이미지로 Detector 학습
   - bbox 어노테이션 품질 우수

3. **데이터 필터링**
   - 테스트 환경과 유사한 3~4개 bbox만 사용
   - 모든 알약 bbox 수집

4. **빠른 실험 사이클**
   - 실패 → 원인 분석 → 방향 전환
   - W&B로 실험 추적

---

### 16. 향후 개선 방향 (1장)

| 영역 | 개선안 |
|------|--------|
| Classifier | EfficientNet 앙상블 |
| Detector | 앙상블 NMS/WBF |
| 데이터 | 더 정밀한 bbox 라벨링 |
| 추론 | 학습/추론 imgsz 일치 |

**현재 한계**:
- 다른 팀 최고 점수: 0.99
- mAP@[0.75:0.95] 기준에서 bbox 정밀도 더 필요

---

### 17. 결론 (1장)

**최종 성과**:
```
Baseline 0.815 → Best 0.96703 (+18.7%)
```

**핵심 메시지**:
1. 문제 분석 → 적합한 아키텍처 선택
2. End-to-end의 한계 인식 → 2-Stage 전환
3. 데이터 품질 > 데이터 양
4. 빠른 실험과 피드백의 중요성

---

### 18. Q&A (1장)

```
감사합니다

GitHub: github.com/Jin94-ai/codeit_team8_project1
```

---

## 발표 시간 배분 (15분)

| 섹션 | 슬라이드 | 시간 |
|------|---------|------|
| 프로젝트 개요 | 1-3 | 2분 |
| 데이터 분석 | 4-5 | 2분 |
| 접근 방식 변화 | 6 | 2분 |
| 모델 아키텍처 | 7-10 | 4분 |
| 실험 결과 | 11-12 | 2분 |
| 팀 협업 | 13-14 | 1.5분 |
| 결론 | 15-17 | 1.5분 |

---

## 주요 시각 자료 (권장)

1. **점수 추이 그래프**: 0.815 → 0.96703
2. **2-Stage 아키텍처 다이어그램**: Detector → Classifier 흐름
3. **데이터 분포**: 학습(74) vs 테스트(196) 클래스
4. **협업 통계**: 커밋/PR/일지 숫자
5. **추론 파이프라인**: 입력 → 검출 → 크롭 → 분류 → 출력

---

## 예상 질문 & 답변

### Q1. 왜 2-Stage를 선택했나요?
> 학습 데이터 74개 클래스 vs 테스트 196개 클래스 불일치 때문입니다.
> Detector를 단일 클래스로 학습하면 모든 알약 위치를 검출할 수 있고,
> Classifier는 학습된 74개 클래스만 분류하면 됩니다.

### Q2. ConvNeXt를 선택한 이유는?
> ImageNet pretrained 모델로 강력한 특징 추출이 가능합니다.
> YOLO-cls 대비 분류 정확도가 0.920 → 0.963으로 향상되었습니다.

### Q3. AIHub 데이터는 어떻게 활용했나요?
> Detector 학습에만 사용했습니다.
> 핵심은 모든 알약의 bbox를 수집하여 Detector가 위치를 잘 학습하도록 한 것입니다.
> 기존 코드는 타겟 클래스 폴더만 스캔하여 bbox가 누락되었습니다.

### Q4. 가장 큰 성능 향상 요인은?
> 2-Stage Pipeline 도입입니다. (+0.230)
> 이후 ConvNeXt 도입 (+0.043), bbox 로직 수정 (+0.004)가 추가 개선에 기여했습니다.

### Q5. 실패 사례에서 배운 점은?
> imgsz 1280 실험에서 학습/추론 설정 일치의 중요성을 배웠습니다.
> 콤보 데이터 실험에서 미라벨링 객체가 성능을 저하시킨다는 것을 확인했습니다.

---

## 발표 스크립트 핵심 포인트

### 오프닝 (30초)
"안녕하세요, 코드잇 8팀 Health Eat 프로젝트 발표를 시작하겠습니다.
저희 팀은 3주간 AI 알약 인식 프로젝트를 진행하여
Baseline 0.815에서 0.96703까지 18.7% 성능 개선을 달성했습니다."

### 핵심 메시지 (반복)
"저희 프로젝트의 핵심은 **End-to-end의 한계를 인식하고 2-Stage Pipeline으로 전환**한 것입니다."

### 클로징 (30초)
"감사합니다. 질문 있으시면 말씀해 주세요."

---

## 파일 생성 완료

이 문서를 기반으로 PPT 또는 PDF 발표자료를 제작하시면 됩니다.
