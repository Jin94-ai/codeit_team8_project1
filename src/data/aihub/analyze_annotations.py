"""
AIHub ë‹¨ì¼ê²½êµ¬ì•½ì œ ë¼ë²¨ë§ ë°ì´í„°(ZIP) ë¶„ì„

ì‚¬ìš©ë²•:
    python -m src.data.aihub.analyze_annotations

êµ¬ì¡°:
data/166.ì•½í’ˆì‹ë³„.../01.ë°ì´í„°/1.Training/ë¼ë²¨ë§ë°ì´í„°/ë‹¨ì¼ê²½êµ¬ì•½ì œ 5000ì¢…/
â”œâ”€â”€ TL_1_ë‹¨ì¼.zip
â”œâ”€â”€ TL_2_ë‹¨ì¼.zip
â””â”€â”€ ... (81ê°œ ZIP)

ì¶œë ¥:
- TL í´ë”ë³„ TARGET_CLASSES í¬í•¨ í˜„í™©
- ë‹¤ìš´ë¡œë“œ ì¶”ì²œ ëª©ë¡ (ì´ë¯¸ì§€ í´ë”)
"""
import json
import sys
import zipfile
from pathlib import Path
from collections import defaultdict

# ì§ì ‘ ì‹¤í–‰ ì‹œ import ê²½ë¡œ ì¶”ê°€
if __name__ == "__main__":
    sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from src.data.aihub.config import TARGET_CLASSES, dl_idx_to_k_code


# ë¼ë²¨ë§ ë°ì´í„° ê²½ë¡œ
LABEL_DIR = Path("data/166.ì•½í’ˆì‹ë³„ ì¸ê³µì§€ëŠ¥ ê°œë°œì„ ìœ„í•œ ê²½êµ¬ì•½ì œ ì´ë¯¸ì§€ ë°ì´í„°/01.ë°ì´í„°/1.Training/ë¼ë²¨ë§ë°ì´í„°/ë‹¨ì¼ê²½êµ¬ì•½ì œ 5000ì¢…")


def find_zip_files(base_dir: Path) -> list:
    """TL_*.zip íŒŒì¼ ëª©ë¡ ì°¾ê¸°"""
    zip_files = list(base_dir.glob("TL_*_ë‹¨ì¼.zip"))
    return sorted(zip_files, key=lambda x: int(x.name.split('_')[1]))


def analyze_zip_file(zip_path: Path) -> dict:
    """
    ë‹¨ì¼ ZIP íŒŒì¼ ë‚´ annotation ë¶„ì„ (ì••ì¶• í•´ì œ ì—†ì´)

    Returns:
        {
            'name': 'TL_1_ë‹¨ì¼',
            'total_files': 1000,
            'target_classes': {'1899': 50, '2482': 30, ...},
            'non_target_count': 500
        }
    """
    result = {
        'name': zip_path.stem,
        'total_files': 0,
        'target_classes': defaultdict(int),
        'non_target_count': 0
    }

    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            json_files = [f for f in zf.namelist() if f.endswith('.json')]

            for json_file in json_files:
                result['total_files'] += 1

                try:
                    with zf.open(json_file) as f:
                        data = json.load(f)

                    # dl_idx ì¶”ì¶œ (ì—¬ëŸ¬ êµ¬ì¡° ì§€ì›)
                    dl_idx = None

                    # êµ¬ì¡° 1: images[0].dl_idx
                    if 'images' in data and data['images']:
                        dl_idx = str(data['images'][0].get('dl_idx', ''))

                    # êµ¬ì¡° 2: categories[0].id
                    if not dl_idx and 'categories' in data and data['categories']:
                        dl_idx = str(data['categories'][0].get('id', ''))

                    # êµ¬ì¡° 3: annotations[0].category_id
                    if not dl_idx and 'annotations' in data and data['annotations']:
                        dl_idx = str(data['annotations'][0].get('category_id', ''))

                    if dl_idx and dl_idx in TARGET_CLASSES:
                        result['target_classes'][dl_idx] += 1
                    elif dl_idx:
                        result['non_target_count'] += 1

                except Exception:
                    continue

    except zipfile.BadZipFile:
        print(f"\n  ê²½ê³ : {zip_path.name} - ì†ìƒëœ ZIP íŒŒì¼")

    return result


def print_analysis_report(tl_results: list):
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("AIHub ë‹¨ì¼ê²½êµ¬ì•½ì œ ë¼ë²¨ë§ ë¶„ì„ ê²°ê³¼")
    print("=" * 70)

    # TARGET_CLASSESê°€ ìˆëŠ” TL í´ë”ë§Œ í•„í„°ë§
    target_tl = [r for r in tl_results if r['target_classes']]

    if not target_tl:
        print("\nâŒ TARGET_CLASSESë¥¼ í¬í•¨í•œ TL íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return [], set()

    # ê²°ê³¼ ì •ë ¬ (TARGET í´ë˜ìŠ¤ ìˆ˜ ê¸°ì¤€)
    target_tl.sort(key=lambda x: len(x['target_classes']), reverse=True)

    print(f"\nì´ {len(tl_results)}ê°œ TL ì¤‘ {len(target_tl)}ê°œì—ì„œ TARGET í´ë˜ìŠ¤ ë°œê²¬\n")

    # TLë³„ ìƒì„¸ ì •ë³´
    all_found_classes = set()
    recommended = []

    for tl in target_tl:
        class_count = len(tl['target_classes'])
        sample_count = sum(tl['target_classes'].values())
        all_found_classes.update(tl['target_classes'].keys())

        # TL ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: TL_1_ë‹¨ì¼ -> 1)
        tl_num = tl['name'].split('_')[1]

        print(f"ğŸ“ {tl['name']}")
        print(f"   TARGET í´ë˜ìŠ¤: {class_count}ê°œ, ìƒ˜í”Œ: {sample_count}ê°œ")

        # ìƒìœ„ 5ê°œ í´ë˜ìŠ¤ í‘œì‹œ
        top_classes = sorted(tl['target_classes'].items(), key=lambda x: -x[1])[:5]
        class_str = ", ".join([f"{c}({n})" for c, n in top_classes])
        print(f"   ì£¼ìš” í´ë˜ìŠ¤: {class_str}")

        if tl['non_target_count'] > 0:
            print(f"   (non-target: {tl['non_target_count']}ê°œ)")
        print()

        recommended.append(tl_num)

    # ìš”ì•½
    print("=" * 70)
    print("ìš”ì•½")
    print("=" * 70)
    print(f"ë°œê²¬ëœ TARGET í´ë˜ìŠ¤: {len(all_found_classes)}/56ê°œ")

    missing = TARGET_CLASSES - all_found_classes
    if missing:
        print(f"\në¯¸ë°œê²¬ í´ë˜ìŠ¤ ({len(missing)}ê°œ):")
        for dl_idx in sorted(missing, key=int)[:10]:
            print(f"  - {dl_idx} ({dl_idx_to_k_code(dl_idx)})")
        if len(missing) > 10:
            print(f"  ... ì™¸ {len(missing) - 10}ê°œ")

    # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¶”ì²œ
    print(f"\n" + "=" * 70)
    print("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¶”ì²œ")
    print("=" * 70)
    print(f"ë‹¤ìŒ TS í´ë”ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
    print(f"  TS_{', TS_'.join(recommended)}")
    print(f"\nì´ {len(recommended)}ê°œ í´ë”")

    return recommended, all_found_classes


def save_results(tl_results: list, recommended: list, found_classes: set):
    """ë¶„ì„ ê²°ê³¼ JSON ì €ì¥"""
    output_path = Path("data/ts_analysis_result.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # í´ë˜ìŠ¤ë³„ ì–´ë–¤ TLì— ìˆëŠ”ì§€ ë§¤í•‘
    class_to_tl = defaultdict(list)
    for tl in tl_results:
        if tl['target_classes']:
            tl_num = tl['name'].split('_')[1]
            for cls in tl['target_classes']:
                class_to_tl[cls].append({
                    'tl': tl_num,
                    'count': tl['target_classes'][cls]
                })

    result = {
        'recommended_image_folders': [f"TS_{num}" for num in recommended],
        'found_target_classes': sorted(found_classes, key=int),
        'missing_target_classes': sorted(TARGET_CLASSES - found_classes, key=int),
        'class_locations': {
            cls: class_to_tl[cls] for cls in sorted(found_classes, key=int)
        },
        'tl_folder_details': [
            {
                'name': tl['name'],
                'image_folder': f"TS_{tl['name'].split('_')[1]}",
                'target_class_count': len(tl['target_classes']),
                'target_sample_count': sum(tl['target_classes'].values()),
                'classes': dict(tl['target_classes'])
            }
            for tl in tl_results if tl['target_classes']
        ]
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nê²°ê³¼ ì €ì¥: {output_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("AIHub ë‹¨ì¼ê²½êµ¬ì•½ì œ ë¼ë²¨ë§ ë¶„ì„")
    print("=" * 70)
    print(f"ë¶„ì„ ëŒ€ìƒ: {LABEL_DIR}")
    print(f"TARGET í´ë˜ìŠ¤: {len(TARGET_CLASSES)}ê°œ")

    if not LABEL_DIR.exists():
        print(f"\nâŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {LABEL_DIR}")
        print("\n[ì‚¬ìš©ë²•]")
        print("1. AIHubì—ì„œ 'ë‹¨ì¼ê²½êµ¬ì•½ì œ 5000ì¢…' ë¼ë²¨ë§ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("2. ìœ„ ê²½ë¡œì— TL_*_ë‹¨ì¼.zip íŒŒì¼ ë°°ì¹˜")
        print("3. ë‹¤ì‹œ ì‹¤í–‰")
        return

    # ZIP íŒŒì¼ ì°¾ê¸°
    zip_files = find_zip_files(LABEL_DIR)

    if not zip_files:
        print(f"\nâŒ TL_*_ë‹¨ì¼.zip íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\në°œê²¬ëœ ZIP íŒŒì¼: {len(zip_files)}ê°œ")

    # ê° ZIP íŒŒì¼ ë¶„ì„
    tl_results = []
    for i, zip_path in enumerate(zip_files):
        print(f"\rë¶„ì„ ì¤‘: {i+1}/{len(zip_files)} - {zip_path.name}", end="", flush=True)
        result = analyze_zip_file(zip_path)
        tl_results.append(result)

    print()  # ì¤„ë°”ê¿ˆ

    # ê²°ê³¼ ì¶œë ¥
    recommended, found_classes = print_analysis_report(tl_results)

    # ê²°ê³¼ ì €ì¥
    if recommended:
        save_results(tl_results, recommended, found_classes)


if __name__ == "__main__":
    main()
